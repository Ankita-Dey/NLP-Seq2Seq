{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final-A_20111013_Phase4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvOfz2D-KvI3"
      },
      "source": [
        "1st submission 30 epoch took 4+ hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITxpwuarAj79"
      },
      "source": [
        "* Mount google drive so that files can be read and written to\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4mIsB9tO_0I",
        "outputId": "73ee9fa4-052b-4f16-96be-91e10a85a8c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGSr7CIUAGoT"
      },
      "source": [
        "* Download and import indicnlp which is used for hindi text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxMINZkMQODq",
        "outputId": "130c689c-6a7d-40c0-cef5-2f561d6cbd12"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "\n",
        "from indicnlp.normalize.indic_normalize import BaseNormalizer\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1271, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1271/1271), 9.56 MiB | 15.71 MiB/s, done.\n",
            "Resolving deltas: 100% (654/654), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 41.82 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry9yYgNLBMmf"
      },
      "source": [
        "*   Import all necessary packages including pytorch\n",
        "*   csv, string, random are python inbuilt libraries.\n",
        "*   csv is used to read the csv into a 2d array\n",
        "*   string is used to do string processing on the text \n",
        "*   random is used to generate pseudo-random number or pick something pseudo-randomly.\n",
        "*   pytorch is used for the sequence to sequence model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ikitpf5NS1nX",
        "outputId": "af8a4bd8-5697-490b-dc03-de546e673d6b"
      },
      "source": [
        "import csv\n",
        "import string\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nSEED = 777\\n\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\ntorch.cuda.manual_seed(SEED)\\ntorch.backends.cudnn.benchmark = True'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVC8APfBZgG"
      },
      "source": [
        "\n",
        "* Path to train.csv which contains training set --> traincsv\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Fe894mUlSY"
      },
      "source": [
        "traincsv = \"drive/MyDrive/AssignmentNLP/train.csv\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZhO9N18CBne"
      },
      "source": [
        "\n",
        "\n",
        "* Store training data in 2d list --> data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MajvyXjPcbK",
        "outputId": "9a31324d-6e32-4f8a-ebf7-33eea14c81a0"
      },
      "source": [
        "with open(traincsv, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "print(len(data))\n",
        "data = data[1:]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvEdx085Egrz"
      },
      "source": [
        "\n",
        "\n",
        "* Remove all punctuations from hindi and english text\n",
        "* Remove empty strings\n",
        "* remove all sentences from hindi column which has only english alphabets or numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo9Na0VxPf5L",
        "outputId": "f63357e7-17f1-4fae-8081-14adb820db2a"
      },
      "source": [
        "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
        "c = 0\n",
        "row = 0\n",
        "while(row<len(data)):\n",
        "    # print(row)\n",
        "    texts_hi = str(data[row][1]).translate(translate_table).strip().lower()\n",
        "    texts_en = str(data[row][2]).translate(translate_table).strip().lower()\n",
        "    \n",
        "    if texts_hi=='' or texts_en=='' or texts_hi in string.punctuation or texts_en in string.punctuation or len(texts_hi)>100:\n",
        "        c+=1\n",
        "        data.pop(row)\n",
        "        \n",
        "    else:\n",
        "        data[row][1] = texts_hi  # texts has no punctuations\n",
        "        data[row][2] = texts_en\n",
        "\n",
        "        tmp = texts_hi.replace(\" \",\"\")\n",
        "        cnt = 0\n",
        "        for i in tmp:\n",
        "            if (i>='a' and i<='z') or i.isnumeric():\n",
        "                cnt+=1\n",
        "        if cnt == len(tmp):\n",
        "            data.pop(row)\n",
        "            c+=1\n",
        "        else:\n",
        "            row+=1\n",
        "\n",
        "print(c, len(data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13128 89194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjM-TKusE3_E"
      },
      "source": [
        "* Normalize hindi texts (I can't read hindi or understand hindi grammar and spellings so I didn't understand what normalization exactly does but seeing the documentation, I thought it helps make the spellings and text consistent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlpErpn7Qpse"
      },
      "source": [
        "normalizer = BaseNormalizer(\"hi\", remove_nuktas=False)\n",
        "\n",
        "row = 0\n",
        "while(row<len(data)):\n",
        "    data[row][1]=normalizer.normalize(str(data[row][1]))\n",
        "    row+=1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VScCGOi0G0K9"
      },
      "source": [
        "\n",
        "*   Tokenize hindi and english (both using indicnlp)\n",
        "*   Only add tokens that are present 3 times atleast in the dataset.\n",
        "*   Build vocabulary of hindi and english text with tokens present atleast twice in training data using 4 dictionaries:\n",
        "\n",
        "> *   English token to unique index --> eng_vocab_stoi\n",
        "*   Unique index to english token   --> eng_vocab_itos\n",
        "*   Hindi token to unique index     --> hin_vocab_stoi\n",
        "*   Unique index to hindi token     --> hin_vocab_itos\n",
        "\n",
        "\n",
        "\n",
        "*   Next create training data (2D list having senteces represented by indices) for hindi and english --> hin_train, eng_train \n",
        "*   In the training data, sentences are sorted primarily according to the length of hindi sentences, and secondarily according to length of english sentences (so that less padding is required later during creation of batches)\n",
        "*   Convert the tokens in the sentences to their respective indices\n",
        "*   Put special index 0 (representing unknown) for tokens which are not present in vocabulary (because they are seen only once)\n",
        "*   Append initial and end tokens to all hindi and english sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DchS6Fi0Gbs_"
      },
      "source": [
        "eng_vocab_itos = {1:\"<pad>\", 2:\"<sos>\", 3:\"<eos>\", 0:\"<ukn>\"}\n",
        "eng_vocab_stoi = {\"<pad>\":1, \"<sos>\":2, \"<eos>\":3, \"<ukn>\":0}\n",
        "hin_vocab_itos = {1:\"<pad>\", 2:\"<sos>\", 3:\"<eos>\", 0:\"<ukn>\"}\n",
        "hin_vocab_stoi = {\"<pad>\":1, \"<sos>\":2, \"<eos>\":3, \"<ukn>\":0}\n",
        "eng_seen=set()\n",
        "hin_seen=set()\n",
        "tokenized = []\n",
        "eng_train = []\n",
        "hin_train = []\n",
        "eng_count = 4\n",
        "hin_count = 4\n",
        "\n",
        "def CreateVocab(tokens, seen, vocab_stoi, vocab_itos, count):\n",
        "    for wrd in tokens:\n",
        "        if wrd not in seen:\n",
        "            seen.add(wrd)\n",
        "        elif wrd not in vocab_stoi:\n",
        "            vocab_stoi[wrd] = count\n",
        "            vocab_itos[count] = wrd\n",
        "            count+=1\n",
        "    return count\n",
        "\n",
        "def CreateTrain(tokens, vocab_stoi):\n",
        "    sentence = [2]\n",
        "    for wrd in tokens:\n",
        "        if wrd in vocab_stoi:\n",
        "            sentence.append(vocab_stoi[wrd])\n",
        "        else:\n",
        "            sentence.append(0)\n",
        "    sentence.append(3)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "for i in data:\n",
        "    eng_tokens = indic_tokenize.trivial_tokenize(i[2])\n",
        "    hin_tokens = indic_tokenize.trivial_tokenize(i[1])\n",
        "    tokenized.append([hin_tokens, eng_tokens])\n",
        "    eng_count = CreateVocab(eng_tokens, eng_seen, eng_vocab_stoi, eng_vocab_itos, eng_count)\n",
        "    hin_count = CreateVocab(hin_tokens, hin_seen, hin_vocab_stoi, hin_vocab_itos, hin_count)\n",
        "\n",
        "tokenized.sort(key=lambda x: (len(x[0]), len(x[1])), reverse = True)\n",
        "\n",
        "\n",
        "for i in tokenized:\n",
        "    eng_sentence = CreateTrain(i[1], eng_vocab_stoi)\n",
        "    hin_sentence = CreateTrain(i[0], hin_vocab_stoi)\n",
        "    if len(eng_sentence)>2 and len(hin_sentence)>2:\n",
        "        eng_train.append(eng_sentence)\n",
        "        hin_train.append(hin_sentence)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKCugfvoQD9r"
      },
      "source": [
        "\n",
        "\n",
        "*   Check vocabulary size and training data size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L2o3madoNmm",
        "outputId": "b21912f6-46ec-4fef-ac65-7d190a94f3b6"
      },
      "source": [
        "len(eng_vocab_itos), len(hin_vocab_itos), len(eng_train), len(hin_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14964, 16360, 89194, 89194)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq7AmYCAQL7V"
      },
      "source": [
        "\n",
        "\n",
        "* A random sentence in training data after all its tokens are represented by corresponding indices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv2tWu8EYNLW",
        "outputId": "2373a1c9-3543-4782-deec-04199c1b633f"
      },
      "source": [
        "eng_train[434]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 5,\n",
              " 20,\n",
              " 171,\n",
              " 2313,\n",
              " 21,\n",
              " 78,\n",
              " 1748,\n",
              " 21,\n",
              " 5,\n",
              " 54,\n",
              " 1673,\n",
              " 31,\n",
              " 245,\n",
              " 1494,\n",
              " 63,\n",
              " 33,\n",
              " 1831,\n",
              " 776,\n",
              " 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsGwaUskQbaI"
      },
      "source": [
        "\n",
        "\n",
        "*   device is set to cuda so that GPU can be used, if available\n",
        "*   On doing so operations on tensors are done by the GPU (tensor is n dimensional arrays on which numeric compuatations can be done optimally, data passed to encoder will be of datatype, tensor)\n",
        "*   Training data is split into batches with 32 sentences in one batch\n",
        "*   Each column in the batch is an individual sentence with extra padding given at the end of shorter sentences so that each batch has same length equal to length of longest sentence in that batch (sorting by length of sentences was done earlier to reduce the need of padding)\n",
        "*   Each batch is converted to tensor\n",
        "*   At each time step, one row of the batch is sent, in the next time step the next row of the batch is sent, and so on. In simple words, single token of multiple sentences are being passed to encoder at the same time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omLtsHrctnB3"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "eng_batch = []\n",
        "hin_batch = []\n",
        "\n",
        "def CreateBatch(train, batch):\n",
        "    k = 0\n",
        "    while(k<len(train)):\n",
        "        tmp_train = train[k:k+32]\n",
        "        cur_batch = []\n",
        "\n",
        "        for i in range(len(max(tmp_train, key=len))):\n",
        "            cur_idx = []\n",
        "            for j in range(len(tmp_train)):\n",
        "                if len(tmp_train[j])>i:\n",
        "                    cur_idx.append(tmp_train[j][i])\n",
        "                else:\n",
        "                    cur_idx.append(1)\n",
        "            cur_batch.append(cur_idx)\n",
        "        batch.append(torch.tensor(cur_batch))\n",
        "        k = k+32\n",
        "\n",
        "CreateBatch(eng_train, eng_batch)\n",
        "CreateBatch(hin_train, hin_batch)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz0ETUOoWfTV"
      },
      "source": [
        "\n",
        "\n",
        "*   The size of last batch might not be equal to batch size, so remove that\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6t_5dG0KG2n"
      },
      "source": [
        "eng_batch = eng_batch[:-1]\n",
        "hin_batch = hin_batch[:-1]\n",
        "\n",
        "len(eng_batch), len(hin_batch)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9ztWbt7W_rn"
      },
      "source": [
        "\n",
        "\n",
        "*   Check a random batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDs5gqQCjU8B",
        "outputId": "a5d35959-1567-4c7d-c972-e66e944708f3"
      },
      "source": [
        "(hin_batch[-1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2],\n",
              "        [ 1020, 15092,   555,   102,  3549, 12444,  1679,   996,  1608,   194,\n",
              "           212,  7290,   963,    41,  1025, 16279, 16284,  1920,  5278,   270,\n",
              "             0,  3398,   146,     0,  4423, 11924, 15080,     0,  6129,  2861,\n",
              "          7363,     0],\n",
              "        [    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-pa_bYUMRB6"
      },
      "source": [
        "*   Initialize the specifications of the encoder and decoder for sequence to sequence model\n",
        "*   The encoder and decoder input size must be specified as the vocabulary size of the source and target language (i.e. hindi and english vocabulary size) respectively --> input_size_encoder, input_size_decoder\n",
        "*   This would be the size of one hot encoder in which each vector represents a unique token given by the index where it is 1 (rest all indices are 0)\n",
        "*   Each token of a vocabulary is converted to a dense representation of words, that captures semantic information of the words in the vocabulary of the training set, called word embedding for which we have to specify the embedding size --> embedding_size\n",
        "*   No pre-trained embedding is used\n",
        "*   Basically hidden state and cell states are two output of LSTM cell which together constitute the context vector. This context vector (fixes size)captures the essence of the source sequences and is given to decoder along with target language (english) vocabulary to get translated sentences. Size of hidden state is specified --> hidden_state\n",
        "*   Number of LSTM layers used for encoder and decoder is specified. If n, then n context vectors are obtained --> num_layers\n",
        "*   Dropout is a regularization technique of ignoring random neurons during training to prevent overfitting. The probability of ignoring each neuron is specified --> dropout.\n",
        "* [Reference used](https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG6k3pM5v3qH"
      },
      "source": [
        "input_size_encoder = len(hin_vocab_itos)\n",
        "input_size_decoder = len(eng_vocab_itos)\n",
        "embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "dropout = 0.5"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-StNQsWMyJg"
      },
      "source": [
        "* Now that all arguments passed to encoder have been specified, encoder LSTM is created.\n",
        "* All the neural network functionality (including the various losses) are created using pytorch nn package. The nn package defines a set of Modules, which are roughly equivalent to neural network layers. It inputs and outputs tensors (which is why the batches given as input was converted to tensor earlier).\n",
        "* Since custom encoder is created, nn.Module is subclassed and forward method defined. In forward method Tensor of input data is accepted and Tensor of output data is returned. Modules defined in the constructor as  well as arbitrary operators on Tensors can be used in forward method.\n",
        "* Using the constructor, when a EncoderLSTM object is created, the attributes of the class EncoderLSTM are initialized, which are then used in forward method.   \n",
        "* nn.Embedding takes a list of indices as input, and outputs the corresponding word embeddings.\n",
        "* Embedding matrix is then given to the LSTM (nn.LSTM is used) which additionally uses previous time step's hidden state and cell state to produce hidden state and cell state that will be used by the LSTM in next time step.\n",
        "* Dimension of all the tensors are given in code. (tensor.shape can be used to get the dimension of the tensor)\n",
        "* [Reference used](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ0Q9NKkt6mJ",
        "outputId": "b3127723-a47d-4e9a-9411-ffb16489ee41"
      },
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.tag = True ##\n",
        "    \n",
        "    # Dimension of embedding neural network --> (input_size, embedding_size)'s dimension\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    \n",
        "    # Dimension of lstm cell neural network --> (hidden_size, cs)'s dimension\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    # input x is a vector of indices corresponding to tokens of current sentence (x.shape=sequence length, batch size)\n",
        "\n",
        "    # embedding.shape = (sequence length, batch size, embedding_size) because now each x is also mapped to a D dimensional embedding.\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    \n",
        "    # outputs.shape = (sequence length, batch size, hidden_size)\n",
        "    # hidden_state.shape = (num_layers, batch size, hidden_size)\n",
        "    # cell_state.shape = (num_layers, batch size, hidden_size)\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    # (hidden_state, cell_state) is the context vector (It will be used by decoder later)\n",
        "    return hidden_state, cell_state\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, embedding_size, hidden_size, num_layers, dropout).to(device)\n",
        "print(encoder_lstm)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(16360, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZjV26YVrtWb"
      },
      "source": [
        "* The Context Vector from the Encoder LSTM is provided as the hidden state and cell state for the decoder’s first LSTM.\n",
        "* The same arguments are passed to DecoderLSTM as passed to EncoderLSTM except for an extra argument, output_size. The output_size is same as input_size i.e. target (english) vocabulary size.\n",
        "* The Encoder and Decoder are identical and code for Encoder and Decoder is the same, except that the prediction vector is also returned from the decoder. It has the probability of each token in the vocabulary being the correct prediction for all the tokens of the batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqXLInVyvPS8",
        "outputId": "43ad4ebf-5bbb-4c59-e033-be990ffdf9a2"
      },
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.output_size = output_size\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "    # Unlike Encoder in which all the token index in the same row of a batch is sent at a time, \n",
        "    # in decoder, each token is sent individually at each time step.\n",
        "\n",
        "    x = x.unsqueeze(0)\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "    \n",
        "    \n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # squeeze(0) removes dimension of size 1 \n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state\n",
        "\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, embedding_size, hidden_size, num_layers, dropout, input_size_decoder).to(device)\n",
        "print(decoder_lstm)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(14964, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=14964, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8W0ye0HNWz2"
      },
      "source": [
        "Create Sequence to Sequence model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcXXUifJvP9Z"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "\n",
        "  def forward(self, source, target, tfr=0.5):\n",
        "    batch_size = source.shape[1]\n",
        "\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(eng_vocab_stoi)\n",
        "    \n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
        "    \n",
        "    x = target[0] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
        "      outputs[i] = output\n",
        "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "    return outputs\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9upe-rDNeK6"
      },
      "source": [
        "Initialize model hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "826ThHw5vVxn"
      },
      "source": [
        "# Hyperparameters\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "learning_rate = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = eng_vocab_stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGIbbTja1oGS"
      },
      "source": [
        "* The model is a sequence to sequence model with LSTM Encoder as first part that takes variable length input from source language and gives fixed length context vector that captures information about the sentences. The context vector is given to LSTM decoder which predicts a token at each time step till some terminating condition is reached, thus giving variable length output of target language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj8cib3-2INY",
        "outputId": "b6e1ce7b-456d-4f3a-92c9-b63faea7d604"
      },
      "source": [
        "model"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (Encoder_LSTM): EncoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(16360, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (Decoder_LSTM): DecoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(14964, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=14964, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXwzSyBl0ZKN"
      },
      "source": [
        "epoch_loss = 0.0\n",
        "best_loss = 999999\n",
        "best_epoch = -1\n",
        "# sentence1 = \"कानून मानव जाति के लिए लागू होता है\""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0AQdF4U_ri"
      },
      "source": [
        "Train the model in small number of epoch each time and save the translation to prevent loss of work in case GPU gets disconnected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91f8P-SXDhNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c54ef7-070e-4eba-9445-2f1680099c4a"
      },
      "source": [
        "num_epochs=10\n",
        "sentence1 = \"उससे बदतर हमारे पेशे ने कानून को जटिलता का चोगा पहना दिया है।\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # #added\n",
        "  # checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "  # save_checkpoint(checkpoint)\n",
        "\n",
        "\n",
        "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
        "  # model.eval()\n",
        "  # translated_sentence1 = translate_sentence(model, sentence1, device, max_length=30)\n",
        "  # print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
        "  # ts1.append(translated_sentence1)\n",
        "\n",
        "  model.train(True)\n",
        "  for i in range(len(hin_batch)):\n",
        "    # input = batch.src.to(device)\n",
        "    # target = batch.trg.to(device)\n",
        "    input = hin_batch[i].to(device)\n",
        "    target = eng_batch[i].to(device)\n",
        "\n",
        "    # Pass the input and target for model's forward method\n",
        "    # print(i)\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    # Clear the accumulating gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss value for every epoch\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Update the weights values using the gradients we calculated using bp \n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "    epoch_loss += loss.item()\n",
        "    writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "    if ((epoch - best_epoch) >= 10):\n",
        "      print(\"no improvement in 10 epochs, break\")\n",
        "      break\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch - 1 / 10\n",
            "saving\n",
            "\n",
            "Epoch_Loss - 3.702443838119507\n",
            "\n",
            "Epoch - 2 / 10\n",
            "Epoch_Loss - 3.21035099029541\n",
            "\n",
            "Epoch - 3 / 10\n",
            "Epoch_Loss - 2.762840986251831\n",
            "\n",
            "Epoch - 4 / 10\n",
            "Epoch_Loss - 2.623439073562622\n",
            "\n",
            "Epoch - 5 / 10\n",
            "Epoch_Loss - 2.2964110374450684\n",
            "\n",
            "Epoch - 6 / 10\n",
            "Epoch_Loss - 2.1306848526000977\n",
            "\n",
            "Epoch - 7 / 10\n",
            "Epoch_Loss - 2.025114059448242\n",
            "\n",
            "Epoch - 8 / 10\n",
            "Epoch_Loss - 2.0840301513671875\n",
            "\n",
            "Epoch - 9 / 10\n",
            "Epoch_Loss - 2.130894660949707\n",
            "\n",
            "Epoch - 10 / 10\n",
            "Epoch_Loss - 1.9807953834533691\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XObjCStVZCta"
      },
      "source": [
        "* Load the test hindi data from csv into a 2d array using python inbuilt csv reader.\n",
        "* Preprocess it by normalizing it and removing punctuations like it was done for training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsm6d_jO1bfH"
      },
      "source": [
        "with open(\"drive/MyDrive/AssignmentNLP/hindistatements.csv\", newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    lines = list(reader)\n",
        "\n",
        "normalizer = BaseNormalizer(\"hi\", remove_nuktas=False)\n",
        "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
        "\n",
        "test_sentences = []\n",
        "for line in lines[1:]:\n",
        "    test_sentences.append(normalizer.normalize(str(line[2]).translate(translate_table).strip()))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZJxzk5zxYwJ"
      },
      "source": [
        "* Create translate_sentence function that translates a hindi text to english. \n",
        "* The function first tokenize input hindi sentence, append start and end tokens and create datatype, torch.LongTensor using index of tokens in the sentence.\n",
        "* Then pass the tensor having index of tokens of current sentence to encoder and get context vector (hidden, cell states).\n",
        "* Keep passing most recently obtained translated token to decoder, starting with start of string token. Of course also pass context vector obtained in previous step. Stop when end of string token is predicted by decoder or maximum sentence length is reached.\n",
        "* Keep appending index of english translation from decoder to a list. Convert indices to english tokens using index to string dictionaries created earlier to create english translation of hindi input.\n",
        "* Return the translated sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB_xNQnnxVIN"
      },
      "source": [
        "# def translate_sentence(model, sentence, device, max_length=30):\n",
        "def translate_sentence(model, sentence, max_length=30):\n",
        "    if type(sentence) == str:\n",
        "        tokens = indic_tokenize.trivial_tokenize(sentence)\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, \"<sos>\")\n",
        "    tokens.append(\"<eos>\")\n",
        "\n",
        "    text_to_indices=[]\n",
        "    for token in tokens:\n",
        "        if token in hin_vocab_stoi:\n",
        "            text_to_indices.append(hin_vocab_stoi[token])\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "    \n",
        "    outputs = [eng_vocab_stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == hin_vocab_stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [eng_vocab_itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss): ##\n",
        "    print('saving')\n",
        "    print()\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, '/content/checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'/content/checkpoint-NMT-SD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uDXv2d_ZRPD"
      },
      "source": [
        "* Translate the hindi text to english using trained model by calling translate_sentence function.\n",
        "* Remove unknown tokens for which no mapping has been found and end of string token.\n",
        "* model.eval notifies the model that right now evaluation is taking place. This deactivates dropout.\n",
        "* TreebankWordDetokenizer from nltk reverts the tokenized sentence to the original state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2nOwRHi1joF"
      },
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "model.eval()\n",
        "pred_sentences = []\n",
        "\n",
        "# for idx, i in enumerate(test_sentences):\n",
        "for i test_sentences:\n",
        "  model.eval()\n",
        "  # translated_sentence = translate_sentence(model, i, device, max_length=50)\n",
        "  translated_sentence = translate_sentence(model, i, max_length=50)\n",
        "  pred_sentences.append(TreebankWordDetokenizer().detokenize(translated_sentence).replace('<ukn>','')[:-6].strip())"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qlkT5N8Zg3L"
      },
      "source": [
        "* See the predicted sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUuH_RmDI8BI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fff0114-6224-4913-cc44-1574e19b1d16"
      },
      "source": [
        "(pred_sentences)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who knows they know knows',\n",
              " 'well travel your',\n",
              " 'then they heard me',\n",
              " 'inspirational',\n",
              " 'doesnt',\n",
              " 'and barbara we know a a a time time',\n",
              " 'i never never anywhere',\n",
              " 'and we were here here',\n",
              " 'if we know i i i i',\n",
              " 'shoot them them',\n",
              " 'i call give obey',\n",
              " 'because because is is',\n",
              " 'he was a in',\n",
              " 'accounts a',\n",
              " 'we need need',\n",
              " 'hed wasnt never leave',\n",
              " 'so you you this',\n",
              " 'i had a',\n",
              " 'you dont',\n",
              " 'a',\n",
              " 'and its comes back back back',\n",
              " 'i had a',\n",
              " 'its too too beautiful',\n",
              " 'because i know what',\n",
              " 'noor  more our our and',\n",
              " 'im so',\n",
              " 'i dont believe this',\n",
              " 'they knew we',\n",
              " 'day another one and',\n",
              " 'it doesnt be easy',\n",
              " 'but since i i i i i i',\n",
              " 'grunting grunting grunting',\n",
              " 'i need a',\n",
              " 'gunshots',\n",
              " 'im me me',\n",
              " 'give them one',\n",
              " 'you know what what',\n",
              " 'buddy abracabadbra right right right right right',\n",
              " 'whistling',\n",
              " 'and its a a a',\n",
              " 'dont',\n",
              " 'ill',\n",
              " 'whered long',\n",
              " 'maybe hes in in in',\n",
              " 'if we ride a a a',\n",
              " 'i was escaped the and and and and and and and and and and',\n",
              " 'even narrow people many work',\n",
              " 'here here here',\n",
              " 'you never remember',\n",
              " 'give follow me',\n",
              " 'that was a a',\n",
              " 'ill be back',\n",
              " 'theyre coming',\n",
              " 'but this was my my my',\n",
              " 'but needs needs',\n",
              " 'must maureen to in in in',\n",
              " 'im feel too',\n",
              " 'sir',\n",
              " 'um',\n",
              " 'and thatll do',\n",
              " '',\n",
              " 'because everybody everyone all all',\n",
              " 'that that that youre you',\n",
              " 'better better just',\n",
              " 'one',\n",
              " 'well sounds sounds',\n",
              " 'how you you sir',\n",
              " '',\n",
              " 'applause applause you you you',\n",
              " 'but wanted needed a',\n",
              " 'ive lost son son',\n",
              " 'some some some',\n",
              " 'how how bout',\n",
              " 'take it',\n",
              " 'gunshots',\n",
              " 'okay',\n",
              " 'i always nothing nothing home home',\n",
              " 'useless',\n",
              " 'and millers is the this',\n",
              " 'were not gonna',\n",
              " '',\n",
              " 'laughter theres another one another',\n",
              " 'and it worked',\n",
              " 'how if you right right right',\n",
              " 'i',\n",
              " 'you know',\n",
              " 'no no no civility bombs mugs tattoos phoenix phoenix speer lannisters lannisters popular i',\n",
              " 'override him',\n",
              " 'its wildfire your your',\n",
              " '',\n",
              " 'im a cop',\n",
              " 'its a a a of',\n",
              " 'youre need',\n",
              " 'i need',\n",
              " 'whats the',\n",
              " 'hey',\n",
              " 'chris',\n",
              " 'its',\n",
              " 'and my my my my',\n",
              " 'grunting',\n",
              " 'but its is is is is',\n",
              " 'its not yours friend',\n",
              " '',\n",
              " 'a',\n",
              " 'were going',\n",
              " 'youre my',\n",
              " 'um was ago',\n",
              " 'think about you',\n",
              " 'okay',\n",
              " 'his name name',\n",
              " 'hes not in in home in',\n",
              " 'laughter and and later',\n",
              " 'who is shit',\n",
              " 'and here there',\n",
              " 'you you you of',\n",
              " 'you knew know',\n",
              " 'youre you',\n",
              " 'tales',\n",
              " 'switching they they',\n",
              " 'its our',\n",
              " 'hes listening right',\n",
              " 'well im having a',\n",
              " 'day day years years years years',\n",
              " 'i neither',\n",
              " 'somethings day some some',\n",
              " 'you  my my my',\n",
              " 'targets',\n",
              " 'ironic starts',\n",
              " 'i heard that',\n",
              " 'remember need to to with',\n",
              " 'chlrplng',\n",
              " 'capitan',\n",
              " 'relax',\n",
              " 'were becoming',\n",
              " 'which station everyone to',\n",
              " 'to',\n",
              " 'i just believe me',\n",
              " 'i cant cant',\n",
              " 'lets lets work',\n",
              " '',\n",
              " 'its disgusting  and',\n",
              " 'its is me me',\n",
              " 'ideas',\n",
              " 'heres a a a a',\n",
              " 'if if want',\n",
              " 'no it',\n",
              " 'and i cant think',\n",
              " 'he was in in',\n",
              " 'so youre youre sunita exquisite begging',\n",
              " 'everybodys mans man car man man',\n",
              " '',\n",
              " 'everybodys matter home people and',\n",
              " 'laughter you can go',\n",
              " 'bob',\n",
              " 'a aside   a',\n",
              " 'when home back back back',\n",
              " 'yes maybe should',\n",
              " 'from',\n",
              " 'its bitter my my right right right',\n",
              " 'i know what',\n",
              " 'lets and and and',\n",
              " 'my life is',\n",
              " 'hello',\n",
              " 'and there else be',\n",
              " 'everybodys worked in',\n",
              " 'and i i the',\n",
              " 'ill will you',\n",
              " 'ideas are a',\n",
              " 'o god',\n",
              " 'and connor you you you you',\n",
              " 'you',\n",
              " 'aye',\n",
              " 'silver me',\n",
              " 'i have a',\n",
              " 'where are',\n",
              " 'house birthday my my',\n",
              " 'but were we one in',\n",
              " 'previously it tight',\n",
              " 'harvest to blend for',\n",
              " 'it was',\n",
              " 'mija',\n",
              " 'when i was my my my my my',\n",
              " '',\n",
              " 'loud than',\n",
              " 'hold',\n",
              " 'its like a',\n",
              " 'keep back',\n",
              " 'were having here are here here',\n",
              " 'or was was in in',\n",
              " 'radar means',\n",
              " 'or or or',\n",
              " 'do you a',\n",
              " 'do',\n",
              " 'i cant',\n",
              " 'its is  are are',\n",
              " 'its not not in',\n",
              " 'tell him him',\n",
              " 'whats he',\n",
              " 'its cant cant you here do do here here',\n",
              " 'shipr',\n",
              " 'no its just just',\n",
              " 'hes magician no',\n",
              " 'how bout bout',\n",
              " 'nothing',\n",
              " 'im just just not my',\n",
              " 'wherever means you you you you you',\n",
              " 'i me i i',\n",
              " 'if know my my my',\n",
              " 'were dont do',\n",
              " 'useless',\n",
              " 'even nothing nothing too',\n",
              " 'talked thing talk talk',\n",
              " 'she was everything right was right',\n",
              " 'engage sign isnt',\n",
              " 'its is not is',\n",
              " '',\n",
              " 'he wasnt not that that',\n",
              " 'hes that that',\n",
              " 'ranu can',\n",
              " 'we had our',\n",
              " 'how can we each each',\n",
              " '',\n",
              " 'sorry about what',\n",
              " 'did did you',\n",
              " 'my yoyo i i was and and',\n",
              " 'officer',\n",
              " 'our our our our and',\n",
              " 'they they they',\n",
              " 'knocks',\n",
              " 'scary',\n",
              " 'she was was was was was',\n",
              " 'them in in',\n",
              " 'i know it',\n",
              " 'we need them help',\n",
              " 'sir were cant',\n",
              " 'and wheres why these how allies time unpredictable',\n",
              " 'laughter and and and our our our our',\n",
              " 'and',\n",
              " 'all we together home and and',\n",
              " 'my my my my my my my my my my',\n",
              " 'like',\n",
              " 'okay so',\n",
              " 'and millers comes and and and and and and and and',\n",
              " 'maybe might late',\n",
              " 'dont dont not them',\n",
              " 'whats in',\n",
              " 'a day of of of of for for',\n",
              " 'what a man',\n",
              " 'that that that died',\n",
              " 'mi them frustrated and and and',\n",
              " 'our our our',\n",
              " 'can you help',\n",
              " 'i lied and and and',\n",
              " '',\n",
              " 'it could clear us us',\n",
              " 'my my works is is',\n",
              " 'can can you can',\n",
              " 'and its is it it it it',\n",
              " 'we we do do do do',\n",
              " 'you should to',\n",
              " 'you know know',\n",
              " 'my',\n",
              " 'unauthorized dont remember',\n",
              " 'or stop',\n",
              " 'but you did',\n",
              " 'so was was martini',\n",
              " 'everybodys survival',\n",
              " 'none dont',\n",
              " 'so i me you you you',\n",
              " 'easy',\n",
              " 'grunting',\n",
              " 'theyre themselves people',\n",
              " 'potato talk',\n",
              " 'they stopped them again',\n",
              " 'and else should something',\n",
              " 'you wanna to',\n",
              " 'whenever i i i either anything',\n",
              " 'our can our our',\n",
              " 'im not a',\n",
              " 'theyre good',\n",
              " 'you have',\n",
              " '',\n",
              " 'shes fucking soul',\n",
              " 'we gotta go',\n",
              " 'thats i i in in time work',\n",
              " 'laughter everybody tonight our we our our',\n",
              " 'watch',\n",
              " 'where where says',\n",
              " 'thats',\n",
              " 'henry for for for',\n",
              " 'go',\n",
              " 'applause were were everything all all all',\n",
              " 'knocks',\n",
              " 'i didnt about in',\n",
              " 'our hive were we we we we',\n",
              " 'it was was in in',\n",
              " 'because youre',\n",
              " 'thats called',\n",
              " 'roger',\n",
              " 'where you you',\n",
              " 'elizabeth',\n",
              " 'what',\n",
              " 'gasps',\n",
              " 'you know i i',\n",
              " 'rookie',\n",
              " 'listen all you you you',\n",
              " 'im sorry sorry sorry sorry',\n",
              " 'can man a a a man',\n",
              " 'no if',\n",
              " 'i me for',\n",
              " 'excellent',\n",
              " 'thats that the',\n",
              " 'do',\n",
              " 'how you you',\n",
              " 'so if you know a you you',\n",
              " 'nice good',\n",
              " '',\n",
              " 'never before again again',\n",
              " '10th',\n",
              " 'delta got in in in',\n",
              " 'think about that that',\n",
              " 'its not in',\n",
              " 'sparkplug home house home',\n",
              " 'no',\n",
              " 'forever for',\n",
              " 'lets two days two two',\n",
              " 'she said shell back back',\n",
              " 'danny',\n",
              " 'you need to',\n",
              " 'thats a cop',\n",
              " 'petra',\n",
              " 'but dont dont know know',\n",
              " 'thats why us',\n",
              " 'shell  bodyguard',\n",
              " 'im here here friend',\n",
              " 'hey',\n",
              " 'running fire',\n",
              " 'welcome back in in',\n",
              " 'its is surprisingly really of',\n",
              " 'wheres your daughter',\n",
              " 'no was was your',\n",
              " 'weve right',\n",
              " 'but nothing',\n",
              " 'she knows i i i i',\n",
              " 'itll it be',\n",
              " 'well hes a',\n",
              " 'how you home home home home home home home home',\n",
              " 'name name',\n",
              " 'you',\n",
              " 'hey',\n",
              " 'when we tend about about about',\n",
              " 'how wonder imagine',\n",
              " 'and my my my my my',\n",
              " 'and how how wrap myself my my',\n",
              " 'what',\n",
              " 'wk yeah',\n",
              " 'each example matters is different is time houses',\n",
              " 'you know',\n",
              " 'indu you',\n",
              " 'jack',\n",
              " 'i dont know',\n",
              " 'applause',\n",
              " 'applause i didnt here',\n",
              " 'okja',\n",
              " 'you could help',\n",
              " 'which day day right is',\n",
              " 'say that',\n",
              " 'i think its is is',\n",
              " 'you cant',\n",
              " 'relax you you you',\n",
              " 'and in in of',\n",
              " 'youve already been',\n",
              " 'i know out right',\n",
              " 'they can silences our our',\n",
              " 'thats what he do do do',\n",
              " 'both both both both both',\n",
              " 'already already',\n",
              " 'dont go we we we we',\n",
              " 'but have',\n",
              " 'cmon',\n",
              " 'laughter applause i i i i what',\n",
              " 'to your your your',\n",
              " 'i dont nothing',\n",
              " 'humming',\n",
              " 'dinner lunch my my my my',\n",
              " 'june',\n",
              " 'patroclus missus',\n",
              " 'forever',\n",
              " 'welcome mine mine',\n",
              " 'koba blew escaped',\n",
              " 'im still still',\n",
              " 'can help help',\n",
              " 'i visited klaus two',\n",
              " 'forgot your with my',\n",
              " 'he he',\n",
              " 'and were got on home home',\n",
              " 'and its time',\n",
              " 'mr',\n",
              " 'i dont know',\n",
              " 'ill take it',\n",
              " 'youre get',\n",
              " 'you got life your',\n",
              " 'if if  just',\n",
              " 'door phone in',\n",
              " 'and i was a once',\n",
              " 'what about you',\n",
              " 'were in',\n",
              " 'please forget them',\n",
              " 'im sorry sorry',\n",
              " 'incoming',\n",
              " 'adapt a for for',\n",
              " 'its is  and and and and and and and and',\n",
              " 'everything one one matter one what',\n",
              " 'they were our different',\n",
              " 'and hes mine mine',\n",
              " 'people drones people',\n",
              " 'its is',\n",
              " 'it was too',\n",
              " 'in life in pocket house',\n",
              " '',\n",
              " 'target is is the',\n",
              " '',\n",
              " 'our our our our',\n",
              " 'define your your your',\n",
              " 'hows my my',\n",
              " 'rani',\n",
              " 'i too mine right right right right',\n",
              " 'wheres are',\n",
              " 'and its is',\n",
              " 'i hanging right right right',\n",
              " 'and these cant tries cant cant',\n",
              " 'edward here here',\n",
              " 'it needs us us',\n",
              " 'tell him him him',\n",
              " 'you grounded have',\n",
              " 'ao guys guy surprise fly',\n",
              " 'of course now now',\n",
              " 'but most  of of',\n",
              " 'and its time',\n",
              " 'if you home home home home home home',\n",
              " 'youre a',\n",
              " 'excellent',\n",
              " 'jack',\n",
              " 'whistling',\n",
              " 'switching you you',\n",
              " 'whos',\n",
              " 'i belong my my my my',\n",
              " 'my hearts my everything everything everything everything',\n",
              " 'lets talk talk',\n",
              " 'you want want coven commute gestures favor snap snap',\n",
              " 'lets go',\n",
              " 'okay you take',\n",
              " 'so they in in in in',\n",
              " 'these my my',\n",
              " 'i need you',\n",
              " 'when i i i i and more and',\n",
              " 'and a a',\n",
              " 'slice it some some',\n",
              " 'higher',\n",
              " 'l',\n",
              " 'im enlisted years',\n",
              " 'nice nice',\n",
              " 'donna you have',\n",
              " 'so why tell me',\n",
              " 'laughter we talk talk about about about',\n",
              " 'but if really how right right right',\n",
              " 'not working going work',\n",
              " 'i said what were were',\n",
              " 'can i help',\n",
              " 'i had something something some some',\n",
              " 'what did he',\n",
              " 'ill me my',\n",
              " 'are they',\n",
              " 'if',\n",
              " 'you',\n",
              " 'how',\n",
              " 'dont mind have',\n",
              " 'im going to',\n",
              " 'easy',\n",
              " 'are you ready',\n",
              " 'amélie years years years',\n",
              " 'ghosts i',\n",
              " 'how act you',\n",
              " 'and they were soccer home home',\n",
              " 'now now how we we',\n",
              " 'man',\n",
              " 'he wanted your',\n",
              " 'soon sunday i and and and and and',\n",
              " 'just only my my',\n",
              " 'which that a',\n",
              " 'its is it',\n",
              " 'so here came there',\n",
              " '',\n",
              " 'no no',\n",
              " 'jack grady yo',\n",
              " 'i said you you you',\n",
              " 'chitchat',\n",
              " 'im freaking dad',\n",
              " 'well right we we we',\n",
              " 'hes right',\n",
              " 'its loaded string',\n",
              " 'i asked you him',\n",
              " 'people says we',\n",
              " 'i sure about',\n",
              " 'so where are going',\n",
              " 'a a a a a',\n",
              " 'something something',\n",
              " 'they were',\n",
              " 'grunts',\n",
              " 'so this seems you you you',\n",
              " 'immune this this this',\n",
              " 'sally',\n",
              " 'cerberus with with with with',\n",
              " 'all crash units my of',\n",
              " 'its',\n",
              " 'i didnt here',\n",
              " 'dont',\n",
              " 'steiners maureen was from',\n",
              " 'ender',\n",
              " 'look i dont',\n",
              " 'sobbing im sorry',\n",
              " 'how are you home',\n",
              " 'but really i i',\n",
              " 'i want',\n",
              " 'youre a',\n",
              " 'and was day',\n",
              " 'lots beautiful',\n",
              " 'and yuttho for for for',\n",
              " 'heres my my my my my my',\n",
              " 'move',\n",
              " 'i cant',\n",
              " 'why cant',\n",
              " 'so',\n",
              " 'he came my my my my my my my',\n",
              " 'in came home home home home home home home',\n",
              " 'time time time blonde indias indias klaus casino',\n",
              " 'if you go',\n",
              " 'alice is tells is is',\n",
              " 'maybe maybe i i',\n",
              " 'its my my',\n",
              " 'they them them everything them',\n",
              " 'anybody leader choice any',\n",
              " 'anyone nothing nothing anything',\n",
              " 'but it was was',\n",
              " 'we can we home home home',\n",
              " 'baby not not in',\n",
              " 'if you home home home your home',\n",
              " 'because your  and your',\n",
              " 'help for my',\n",
              " 'i heard a',\n",
              " 'you with with',\n",
              " 'jim not good good good',\n",
              " 'yeah im',\n",
              " 'hes going going',\n",
              " 'you you were a home',\n",
              " 'hes a',\n",
              " 'thats things people our and and our and',\n",
              " 'i think that a a a a',\n",
              " 'and youre a a a a a',\n",
              " 'gunshots',\n",
              " 'now here go here',\n",
              " 'you cannot breathe in in in in in',\n",
              " 'you matter be be be be',\n",
              " 'thanks',\n",
              " 'i wish',\n",
              " 'were dont here here',\n",
              " 'radar  cant cant cant',\n",
              " 'and said say',\n",
              " 'thanks',\n",
              " 'stanleys',\n",
              " 'forever got with',\n",
              " 'he help help',\n",
              " 'with with',\n",
              " 'thanks',\n",
              " 'visited stops stairs',\n",
              " 'nothing',\n",
              " 'override them them',\n",
              " 'look sign no',\n",
              " 'someones anyone you',\n",
              " 'i know love and and and and and',\n",
              " 'do',\n",
              " 'yeah',\n",
              " 'we appreciate everything everything with everything',\n",
              " 'and he said that that',\n",
              " 'just good good',\n",
              " 'dont him to',\n",
              " 'girl',\n",
              " 'act a a',\n",
              " 'god',\n",
              " 'will you son',\n",
              " 'incoming',\n",
              " 'khal wells starts a a a a a',\n",
              " 'you shook',\n",
              " 'whats name name',\n",
              " 'it belonged my my my my',\n",
              " 'relax',\n",
              " 'but you know know',\n",
              " 'i said you',\n",
              " 'oh i have',\n",
              " 'quiet',\n",
              " 'no no amplified amplified ohho bumpy chin dogging dogging dogging span dogging dogging dogging sire sire sire sire sire sire sire slowing sire sire saturn saturn speer saturn saturn sire dash dash dash employees shower ambitious letty',\n",
              " 'theres need need',\n",
              " 'one',\n",
              " 'who whos the with in in',\n",
              " 'laughing',\n",
              " '© grunting nder',\n",
              " 'or you you get go',\n",
              " 'yeah',\n",
              " 'theyre',\n",
              " 'i just need',\n",
              " 'they knows my my my my my my my my',\n",
              " 'and these these these these and',\n",
              " '',\n",
              " 'no i dont',\n",
              " 'you got',\n",
              " 'shaved something some to',\n",
              " 'inhales',\n",
              " 'so here work work work',\n",
              " 'youre on',\n",
              " 'but its birthday',\n",
              " 'and then our our our our and and and and and and',\n",
              " 'they all people people all',\n",
              " 'youre leaving',\n",
              " 'are',\n",
              " '',\n",
              " 'he was for home',\n",
              " 'then play a a a',\n",
              " 'they stay',\n",
              " 'that that that that',\n",
              " 'exhales',\n",
              " 'like like',\n",
              " 'you should me',\n",
              " 'ive done',\n",
              " 'i mean i got',\n",
              " 'im sorry about blend',\n",
              " 'if not there on',\n",
              " 'but still today',\n",
              " 'not yet yet yet',\n",
              " 'its home home',\n",
              " 'i love you',\n",
              " 'when i was right right',\n",
              " 'well that',\n",
              " 'today lot lot lot lot',\n",
              " 'were',\n",
              " 'you do suggesting me',\n",
              " 'and  is is in',\n",
              " 'check',\n",
              " 'loud',\n",
              " 'i should',\n",
              " 'its happening on',\n",
              " 'another',\n",
              " 'eureka',\n",
              " 'easy this this',\n",
              " 'laughter but but i i i',\n",
              " 'oops had a a and',\n",
              " 'and what dating with',\n",
              " 'so play some some',\n",
              " 'we talk to about',\n",
              " 'dont do it',\n",
              " 'dont cry',\n",
              " 'where are',\n",
              " 'relax',\n",
              " 'people people people people',\n",
              " 'what what fuck',\n",
              " 'you you',\n",
              " 'were not',\n",
              " 'sb hilarious beautiful very here',\n",
              " 'and ill ill on',\n",
              " 'because there there there there',\n",
              " 'well itll be',\n",
              " 'so thats is and and and and and and and',\n",
              " 'home your',\n",
              " 'im in',\n",
              " 'its on',\n",
              " 'the continues continues from',\n",
              " '',\n",
              " 'and we we have',\n",
              " 'ow',\n",
              " 'is got with',\n",
              " 'l',\n",
              " 'a of',\n",
              " 'one that that you you you you you you',\n",
              " 'yep surely was simple',\n",
              " 'thats completely completely and and and and',\n",
              " 'ive heard in in',\n",
              " 'im not not you',\n",
              " 'we have what',\n",
              " 'okay',\n",
              " 'thats that pretty a a',\n",
              " 'detected breach',\n",
              " 'together tell him',\n",
              " 'its is for for',\n",
              " 'executioner',\n",
              " 'and then then home home back',\n",
              " 'with with with with with',\n",
              " '',\n",
              " 'imagine requires to a a',\n",
              " 'maxs',\n",
              " 'you didnt',\n",
              " 'its',\n",
              " 'no wait me',\n",
              " 'so',\n",
              " 'hello trevor',\n",
              " 'you son son',\n",
              " 'im telling you you',\n",
              " 'heres heres easy easy',\n",
              " 'no no no civility bombs mugs tattoos phoenix phoenix speer lannisters lannisters popular i',\n",
              " 'dismissed those them them',\n",
              " 'its really really really to',\n",
              " 'why dont you',\n",
              " 'truly in',\n",
              " 'she needs need need',\n",
              " 'because im am this',\n",
              " 'give',\n",
              " 'i asked him him in',\n",
              " 'everyone knows i i do do',\n",
              " 'i didnt my my my my my my my',\n",
              " 'examine me me',\n",
              " 'sorry',\n",
              " 'yeah',\n",
              " 'whats up',\n",
              " 'so religions work people people people',\n",
              " 'my wish a',\n",
              " 'just just not on',\n",
              " 'mine',\n",
              " 'and ears your your your',\n",
              " 'you need need',\n",
              " 'their hundreds',\n",
              " 'how know you',\n",
              " 'stop',\n",
              " 'exhales',\n",
              " 'down',\n",
              " 'appreciate it',\n",
              " 'sort that that',\n",
              " 'good',\n",
              " 'hes was with with with back',\n",
              " 'and heres here here here here',\n",
              " 'forget it it talking',\n",
              " '',\n",
              " 'reload',\n",
              " 'when they knew were were',\n",
              " 'its a a a a in',\n",
              " 'its fixed   and and and and',\n",
              " 'there there',\n",
              " 'i dont you you you',\n",
              " 'its is my my',\n",
              " 'and you you go home home home home',\n",
              " 'what about his',\n",
              " 'i saw me and and and',\n",
              " 'are you',\n",
              " 'chlrplng back in in',\n",
              " 'we could be in',\n",
              " 'you you  and and and and and you you',\n",
              " 'with',\n",
              " 'but maybe was did',\n",
              " 'and we we some',\n",
              " 'applause you you',\n",
              " 'even not here you you',\n",
              " 'ill take a out minute car',\n",
              " 'with mercy together of of',\n",
              " 'cant cant',\n",
              " 'mayday',\n",
              " 'theyre going home home home home',\n",
              " 'come here here here',\n",
              " 'but our our a of',\n",
              " 'tell',\n",
              " 'why',\n",
              " 'why wonder they',\n",
              " 'i for for for for for for',\n",
              " 'theyre fun',\n",
              " 'herc and more more more',\n",
              " 'they wants',\n",
              " 'facebook confirmed software people people',\n",
              " 'but grounded still still',\n",
              " 'littlefinger i helped myself',\n",
              " 'you you ever i',\n",
              " 'thats is watching theyre',\n",
              " 'stay',\n",
              " 'sorry',\n",
              " 'were ready',\n",
              " 'i was a a and',\n",
              " 'apparently that done',\n",
              " 'and when time time lover',\n",
              " 'your',\n",
              " '',\n",
              " 'you know',\n",
              " 'and i sure',\n",
              " 'no but',\n",
              " 'i in a in',\n",
              " 'ignore good good good',\n",
              " 'okay right calm for you',\n",
              " 'we need us',\n",
              " 'accounts are',\n",
              " 'our',\n",
              " 'you you my',\n",
              " 'music music music music',\n",
              " 'no',\n",
              " 'if they less again',\n",
              " 'i sure you',\n",
              " 'i day years days home and',\n",
              " 'ive got now now',\n",
              " 'later in in',\n",
              " 'marry',\n",
              " 'hey',\n",
              " 'fegelein',\n",
              " 'edward is is is is you you',\n",
              " 'these explosion in in in in',\n",
              " 'hows',\n",
              " 'davina says him you to',\n",
              " 'so its is',\n",
              " 'im me me',\n",
              " 'to a',\n",
              " 'lets to',\n",
              " 'i love',\n",
              " '',\n",
              " 'now grounded go',\n",
              " 'sean you calm back back back',\n",
              " 'its a a',\n",
              " 'how many you',\n",
              " 'follow us',\n",
              " 'although this this it this',\n",
              " 'so you know know',\n",
              " 'two two',\n",
              " 'im in in in in in',\n",
              " 'and seductive i i my my',\n",
              " 'ranu can',\n",
              " 'he knew everything right right right',\n",
              " 'whats up',\n",
              " 'she never someone someone  my',\n",
              " 'im sorry sorry sorry sorry',\n",
              " 'but now still i about',\n",
              " 'whats it hells',\n",
              " 'believe',\n",
              " 'maybe you you you',\n",
              " 'its dont dont',\n",
              " 'no but hindi aisle questionnaire questionnaire alignment alignment monsoon worldwide7477 irrational irrational alignment symbolic intensity alignment alignment intensity intensity intensity irrational irrational irrational intensity intensity irrational intensity irrational intensity intensity irrational intensity symbolic symbolic swell telescope dust',\n",
              " '',\n",
              " 'laughter my my my my my my my',\n",
              " 'its remains your your',\n",
              " 'oogway was to',\n",
              " 'shes in in',\n",
              " 'but i thought i i i me',\n",
              " 'we said you home home home home',\n",
              " 'thanks',\n",
              " 'yeah',\n",
              " 'daddys mother dad dad i  overrated  in  in',\n",
              " 'loud speaking',\n",
              " 'who you you',\n",
              " 'ive lied me',\n",
              " 'i i i home diamonds diamonds',\n",
              " 'youre dont',\n",
              " 'hurry time time time sale yanking this',\n",
              " 'its keeps comes and and',\n",
              " 'come',\n",
              " 'executioner',\n",
              " 'and never ever more',\n",
              " 'you always always where',\n",
              " 'what',\n",
              " 'but i me you',\n",
              " 'and they they go go',\n",
              " 'i thought told  a',\n",
              " 'and some some some people some',\n",
              " 'hi',\n",
              " 'laughter i was was',\n",
              " 'yeah lets',\n",
              " 'likewise',\n",
              " 'jessie',\n",
              " 'its little little day',\n",
              " 'but im coming here',\n",
              " 'people people',\n",
              " 'but i think you',\n",
              " 'thats okay',\n",
              " 'and thats distributions and and and and and and',\n",
              " 'chlrplng talking talking',\n",
              " 'go in in',\n",
              " 'shaw we got with with',\n",
              " 'we were together home home together together home',\n",
              " '',\n",
              " '',\n",
              " 'everything worked',\n",
              " 'what',\n",
              " '',\n",
              " 'were ready',\n",
              " 'its is to of of',\n",
              " 'pirates rumbling badly',\n",
              " 'i love apologize boy son',\n",
              " 'ironic',\n",
              " 'stand him',\n",
              " '© prm nder nder',\n",
              " 'if if',\n",
              " 'we might have',\n",
              " '',\n",
              " 'after year year years years',\n",
              " 'sometimes sometimes in in in in in',\n",
              " 'my hearts my',\n",
              " 'theres problem no no lies nor awareness trusted persist boobs alignment dowry what the',\n",
              " 'apparently means only',\n",
              " 'yes',\n",
              " 'so i want',\n",
              " 'people people',\n",
              " 'i know you',\n",
              " 'alan',\n",
              " 'its just',\n",
              " 'and we in in',\n",
              " 'so thats home home home home',\n",
              " 'everybodys in some some  in',\n",
              " 'so what am i',\n",
              " 'think about it',\n",
              " 'we we cant',\n",
              " 'you killed it',\n",
              " 'move',\n",
              " 'wow',\n",
              " 'judith hes needs loves',\n",
              " 'hes a a',\n",
              " 'thats my',\n",
              " 'hey its',\n",
              " 'yes its your',\n",
              " 'lets move this',\n",
              " 'it helped me me',\n",
              " 'too',\n",
              " 'wait',\n",
              " 'another another paradise in',\n",
              " 'a  man',\n",
              " 'im just with with',\n",
              " 'i go go',\n",
              " 'its is another and',\n",
              " 'its a a and and and and',\n",
              " 'he was',\n",
              " 'im sure thats',\n",
              " 'mantis all and and and and and',\n",
              " 'youre you',\n",
              " 'none cant any any',\n",
              " 'would you you a a',\n",
              " 'yeah i got a',\n",
              " 'so',\n",
              " 'but its truly work this this',\n",
              " 'it was a a misunderstanding',\n",
              " 'my life my my',\n",
              " 'why',\n",
              " 'yeah',\n",
              " 'but this was were not',\n",
              " 'it was were hugely were',\n",
              " 'its really a in',\n",
              " 'im a you a',\n",
              " 'its speaking bullshit',\n",
              " 'we were go back back',\n",
              " 'move',\n",
              " 'usually',\n",
              " 'hurry go time',\n",
              " 'treasure',\n",
              " 'nothings magician one a you',\n",
              " 'forever for for for',\n",
              " 'you never never',\n",
              " 'your yourself yourself anything',\n",
              " 'are you my my',\n",
              " 'you you',\n",
              " 'get ahead home home and',\n",
              " 'nap soon work',\n",
              " 'second  people people people people is work',\n",
              " 'hypnosis',\n",
              " 'she was my my',\n",
              " 'gotcha',\n",
              " 'please',\n",
              " 'seriously you',\n",
              " 'a',\n",
              " 'ender i became a',\n",
              " 'why  you you you',\n",
              " 'ill shoot them them',\n",
              " 'lets go',\n",
              " 'but they never not a',\n",
              " 'ive have   of of',\n",
              " 'especially',\n",
              " 'you you out',\n",
              " 'another day another and and and and and and and and',\n",
              " 'stop',\n",
              " 'everything got everything',\n",
              " 'jack',\n",
              " 'one another one more',\n",
              " 'but we needed',\n",
              " 'you know i i i i i',\n",
              " 'well ill talk talk',\n",
              " 'here',\n",
              " 'groans',\n",
              " 'its hilarious important',\n",
              " 'hell not in in',\n",
              " 'that was years',\n",
              " 'what are',\n",
              " 'and deadly   and and and and and and and and and and',\n",
              " 'you to me',\n",
              " 'i told',\n",
              " 'these',\n",
              " 'harvest to to to a',\n",
              " 'and theres is and and and',\n",
              " 'and need forever',\n",
              " 'we can together',\n",
              " 'lynne',\n",
              " 'kerabai were were  and',\n",
              " 'house rendezvous',\n",
              " 'emma',\n",
              " 'theyre just',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PiUSP6y1W1l"
      },
      "source": [
        "* Write the translated english sentences in answer.txt which is submitted for the competition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vakp_fq71sFt"
      },
      "source": [
        "with open(\"drive/MyDrive/AssignmentNLP/answer.txt\", 'w') as f:\n",
        "  for i in pred_sentences:\n",
        "    f.write(i+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB9oMaeEoq7F"
      },
      "source": [
        "References:\n",
        "\n",
        "* https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
        "* https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350\n",
        "* Numerous google search and question answer forum like stackoverflow"
      ]
    }
  ]
}